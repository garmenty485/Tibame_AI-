{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOJFyELe7wsG"
   },
   "source": [
    "Please download the following zip file, extract it and put it under your google \n",
    "drive\n",
    "\n",
    "https://tinyurl.com/bsff25hs\n",
    "\n",
    "In this example, we will perform a binary classification for heavy makeup and no-heavy makeup images coming from CelebA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NApxKm8Z69Mn",
    "outputId": "b1790351-314d-49fc-d33f-e6d9d6c8189e"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ipq4ZTtXfcK-"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RyTXyWEM7IOi",
    "outputId": "be9ee7a3-9015-456d-e1b9-79f230b380bb"
   },
   "outputs": [],
   "source": [
    "# if you mount Google drive correctly, the following commands should be able to executed correctly\n",
    "!ls /content/drive/\n",
    "%cd \"/content/drive/My Drive\"\n",
    "%cd \"heavy_makeup_CelebA_s\"\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZR4O5aWC7qWM",
    "outputId": "dfc3c2fe-0b78-4c89-e113-d6d43b4a33fc"
   },
   "outputs": [],
   "source": [
    "# step 1: preparing HOG feature vectors for heavy makeup & no-heavy makeup classification in training & validation, respectively\n",
    "\n",
    "from skimage import data, color, feature\n",
    "import skimage.data\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "import glob\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# image size before cropping\n",
    "resize_img_w = 256\n",
    "resize_img_h = 256\n",
    "\n",
    "# image size after cropping\n",
    "crop_img_w = 224\n",
    "crop_img_h = 224\n",
    "\n",
    "# feature size for 224x224 image using default HOG settings from sklearn\n",
    "HOG_dim = 54756\n",
    "\n",
    "# the folder of training data in YOUR Google drive\n",
    "training_heavy_makeup_imgs_dir = \"/content/drive/My Drive/heavy_makeup_CelebA_s/train/heavy_makeup/\"\n",
    "training_non_heavy_makeup_imgs_dir = \"/content/drive/My Drive/heavy_makeup_CelebA_s/train/no_heavy_makeup/\"\n",
    "\n",
    "# the folder of validation data in YOUR Google drive\n",
    "validation_heavy_makeup_imgs_dir = \"/content/drive/My Drive/heavy_makeup_CelebA_s/val/heavy_makeup/\"\n",
    "validation_non_heavy_makeup_imgs_dir = \"/content/drive/My Drive/heavy_makeup_CelebA_s/val/no_heavy_makeup/\"\n",
    "\n",
    "# the file lists of training data\n",
    "training_heavy_makeup_img_files = glob.glob(training_heavy_makeup_imgs_dir + '*.jpg')\n",
    "training_no_heavy_makeup_img_files = glob.glob(training_non_heavy_makeup_imgs_dir + '*.jpg')\n",
    "\n",
    "# the file lists of validation data\n",
    "validation_heavy_makeup_img_files = glob.glob(validation_heavy_makeup_imgs_dir + '*.jpg')\n",
    "validation_no_heavy_makeup_img_files = glob.glob(validation_non_heavy_makeup_imgs_dir + '*.jpg')\n",
    "\n",
    "# the number of trainimg images: 1000 for postive and negative, respectively\n",
    "training_heavy_makeup_img_file_num = len(training_heavy_makeup_img_files)\n",
    "training_no_heavy_makeup_img_file_num = len(training_no_heavy_makeup_img_files)\n",
    "\n",
    "# the number of validation images: 1000 for postive and negative, respectively\n",
    "validation_heavy_makeup_img_file_num = len(validation_heavy_makeup_img_files)\n",
    "validation_no_heavy_makeup_img_file_num = len(validation_no_heavy_makeup_img_files)\n",
    "\n",
    "\n",
    "#print(len(training_heavy_makeup_img_files))\n",
    "#print(training_no_heavy_makeup_img_files)\n",
    "#print(validation_heavy_makeup_img_files)\n",
    "#print(validation_no_heavy_makeup_img_files)\n",
    "\n",
    "print(\"the number of heavy makeup images in training folder:{} \".format(training_heavy_makeup_img_file_num))\n",
    "print(\"the number of no heavy makeup images in training folder:{} \".format(training_no_heavy_makeup_img_file_num))\n",
    "\n",
    "print(\"the number of heavy makeup images in validation folder:{}\".format(validation_heavy_makeup_img_file_num))\n",
    "print(\"the number of no heavy makeup images in validation folder:{} \".format(validation_no_heavy_makeup_img_file_num))\n",
    "\n",
    "def img_crop(img, resize_img_w, resize_img_h, target_img_w, target_img_h):\n",
    "  #h, w, c =img.shape\n",
    "  img = cv2.resize(img, (int(resize_img_w), int(resize_img_h)))  \n",
    "  top_y = (resize_img_h-target_img_h)/2\n",
    "  btm_y = resize_img_h-(resize_img_h-target_img_h)/2\n",
    "  left_x = (resize_img_w-target_img_w)/2\n",
    "  right_x = resize_img_w-(resize_img_w-target_img_w)/2\n",
    "  cropped_img = img[int(top_y):int(btm_y),int(left_x):int(right_x) ]\n",
    "  \n",
    "  return cropped_img\n",
    "\n",
    "HOG_postitive_matrix = np.zeros([int(training_heavy_makeup_img_file_num), HOG_dim])\n",
    "HOG_negative_matrix = np.zeros([int(training_no_heavy_makeup_img_file_num), HOG_dim])\n",
    "\n",
    "print(\"start to prepare HOG features for postive training images\")\n",
    "start = timer()\n",
    "for i in range(0,int(training_heavy_makeup_img_file_num)):\n",
    "  #print(\"i={}\".format(i))\n",
    "  img = cv2.imread(training_heavy_makeup_img_files[i])\n",
    "  img_cropped = img_crop(img, resize_img_w, resize_img_h, crop_img_w, crop_img_h)\n",
    "  image = cv2.cvtColor(img_cropped, cv2.COLOR_BGR2GRAY)\n",
    "  hog_vec = feature.hog(image)\n",
    "  #print(hog_vec.shape)\n",
    "  HOG_postitive_matrix[i,:] = hog_vec\n",
    "end = timer()\n",
    "print(\"finish preparing HOG features for postive training images and totally {}-seconds are consumed\".format(end-start))\n",
    "\n",
    "\n",
    "print(\"start to prepare HOG features for negative training images\")\n",
    "start = timer()\n",
    "for i in range(0,int(training_no_heavy_makeup_img_file_num)):\n",
    "  #print(\"i={}\".format(i))\n",
    "  img = cv2.imread(training_no_heavy_makeup_img_files[i])\n",
    "  img_cropped = img_crop(img, resize_img_w, resize_img_h, crop_img_w, crop_img_h)\n",
    "  image = cv2.cvtColor(img_cropped, cv2.COLOR_BGR2GRAY)\n",
    "  hog_vec = feature.hog(image)\n",
    "  #print(hog_vec.shape)\n",
    "  HOG_negative_matrix[i,:] = hog_vec\n",
    "end = timer()\n",
    "print(\"finish preparing HOG features for negative training images and totally {}-seconds are consumed\".format(end-start))\n",
    "\n",
    "# stack HOG postive & negative features vertically\n",
    "training_matrix = np.vstack((HOG_postitive_matrix,HOG_negative_matrix))\n",
    "\n",
    "# initialize the label matrix where top half is positive\n",
    "label_matrix = np.zeros(int(training_heavy_makeup_img_file_num) + int(training_no_heavy_makeup_img_file_num))\n",
    "label_matrix[0:int(training_heavy_makeup_img_file_num)] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QNaCP8fJRv_T",
    "outputId": "1ae562ae-f61c-4139-9ee6-c1e5b8d06127"
   },
   "outputs": [],
   "source": [
    "# step 2: performing (SVM) classifier training using sklearn\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(\"start to perform training\")\n",
    "start = timer()\n",
    "# brute-force for all hyper parameters in training SVM\n",
    "grid = GridSearchCV(LinearSVC(), {'C': [1.0, 2.0, 4.0, 8.0]})\n",
    "grid.fit(training_matrix, label_matrix)\n",
    "grid.best_score_\n",
    "model = grid.best_estimator_\n",
    "model.fit(training_matrix, label_matrix)\n",
    "end = timer()\n",
    "print(\"training is done and totally {}-seconds are consumed\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lDar8VxC8G2w",
    "outputId": "e46d9500-9a0d-4ab8-f39c-c8312579169d"
   },
   "outputs": [],
   "source": [
    "# step 3: accuracy estimation by classifying all the images in the validation folder\n",
    "\n",
    "# for positive validation data\n",
    "TP_num = 0\n",
    "for i in range(0,validation_heavy_makeup_img_file_num): #\n",
    "  img = cv2.imread(validation_heavy_makeup_img_files[i])\n",
    "  img_cropped = img_crop(img, resize_img_w, resize_img_h, crop_img_w, crop_img_h)\n",
    "  image = cv2.cvtColor(img_cropped, cv2.COLOR_BGR2GRAY)\n",
    "  hog_vec = feature.hog(image)\n",
    "  hog_vec = hog_vec.reshape(1,-1)\n",
    "  labels = model.predict(hog_vec)\n",
    "\n",
    "  if int(labels[0]) == 1:\n",
    "      #print(\"this is TP\")\n",
    "      TP_num = TP_num + 1\n",
    "\n",
    "print(\"TP_num={}\".format(TP_num))\n",
    "\n",
    "# for negative validation data\n",
    "TN_num = 0\n",
    "for i in range(0,validation_no_heavy_makeup_img_file_num): #\n",
    "  img = cv2.imread(validation_no_heavy_makeup_img_files[i])\n",
    "  img_cropped = img_crop(img, resize_img_w, resize_img_h, crop_img_w, crop_img_h)  \n",
    "  image = cv2.cvtColor(img_cropped, cv2.COLOR_BGR2GRAY)\n",
    "  hog_vec = feature.hog(image)\n",
    "\n",
    "  #print(hog_vec.shape)\n",
    "  hog_vec = hog_vec.reshape(1,-1)\n",
    "  # perform classification\n",
    "  labels = model.predict(hog_vec)\n",
    "  if int(labels[0]) == 0:\n",
    "    #print(\"this is TP\")\n",
    "    TN_num = TN_num + 1\n",
    "\n",
    "print(\"TN_num={}\".format(TN_num))\n",
    "accuracy = (TP_num+TN_num)/(validation_heavy_makeup_img_file_num + validation_no_heavy_makeup_img_file_num)\n",
    "print(\"accuracy={}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "ZyshwZIiCaLM",
    "outputId": "2510c1b6-dc4f-428e-c1a1-9b42bbe04a5f"
   },
   "outputs": [],
   "source": [
    "# let's classify an image in the validation set\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "image_i = 0\n",
    "\n",
    "img = cv2.imread(validation_heavy_makeup_img_files[image_i])\n",
    "img_cropped = img_crop(img, resize_img_w, resize_img_h, crop_img_w, crop_img_h)\n",
    "image = cv2.cvtColor(img_cropped, cv2.COLOR_BGR2GRAY)\n",
    "hog_vec = feature.hog(image)\n",
    "hog_vec = hog_vec.reshape(1,-1)\n",
    "labels = model.predict(hog_vec)\n",
    "\n",
    "if int(labels[0]) == 1:\n",
    "  print(\"There is a heavy-makeup person inside\")\n",
    "elif int(labels[0]) == 0:\n",
    "  print(\"There is no heavy-makeup person inside\")\n",
    "\n",
    "cv2_imshow(img)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
